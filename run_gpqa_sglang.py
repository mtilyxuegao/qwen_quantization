#!/usr/bin/env python3
"""
Run GPQA evaluation using sglang backend (Zero-shot)
Designed for Qwen3-4B-Instruct and W8A16 quantized models

Usage:
    # Original model
    python run_gpqa_sglang.py --model original
    
    # Quantized model
    python run_gpqa_sglang.py --model w8a16
    
    # Custom configuration
    python run_gpqa_sglang.py \
        --model-name Qwen3-4B-Instruct-2507 \
        --base-url http://127.0.0.1:30000/v1 \
        --variant extended \
        --max-tokens 32768
"""
import sys
import json
import argparse
from pathlib import Path
from openai import OpenAI

# Add parent directory to Python path
sys.path.insert(0, str(Path(__file__).parent))

# Import simple_evals packages
from simple_evals.gpqa_eval import GPQAEval
from simple_evals.types import SamplerBase, SamplerResponse
from simple_evals import common


class SglangSampler(SamplerBase):
    """
    Sglang Backend Sampler
    Supports OpenAI-compatible API
    """
    
    def __init__(
        self, 
        base_url: str, 
        temperature: float | None = None,
        top_p: float | None = None,
        presence_penalty: float | None = None,
        max_tokens: int = 16384,
        seed: int = 1234,
        system_message: str = "You are a helpful assistant."
    ):
        """
        Args:
            base_url: sglang server address, e.g. http://127.0.0.1:30000/v1
            temperature: sampling temperature (None = use model default, 0.0 = greedy)
            top_p: nucleus sampling parameter (None = use model default)
            presence_penalty: presence penalty parameter (None = use model default)
            max_tokens: maximum number of tokens to generate
            seed: random seed (for reproducibility)
            system_message: system prompt message
        """
        # Increase timeout for complex questions (default 600s too short)
        # GPQA questions + max_tokens=16k may take a long time
        self.client = OpenAI(
            base_url=base_url, 
            api_key="dummy",
            timeout=3600.0  # 60 minute timeout
        )
        self.temperature = temperature
        self.top_p = top_p
        self.presence_penalty = presence_penalty
        self.max_tokens = max_tokens
        self.seed = seed
        self.system_message = system_message
    
    def _pack_message(self, content: str, role: str):
        """Pack message into OpenAI format"""
        return {"role": role, "content": content}
    
    def __call__(self, message_list):
        """
        Call sglang backend to generate response
        
        Args:
            message_list: List of messages (not including system message)
            
        Returns:
            SamplerResponse
        """
        # Add system message
        messages = [self._pack_message(self.system_message, "system")] + message_list
        
        # Build request parameters (only pass non-None parameters)
        request_kwargs = {
            "model": "default",  # sglang ignores this parameter
            "messages": messages,
            "max_tokens": self.max_tokens,
            "seed": self.seed
        }
        
        # Only explicitly set parameters will override model defaults
        if self.temperature is not None:
            request_kwargs["temperature"] = self.temperature
        if self.top_p is not None:
            request_kwargs["top_p"] = self.top_p
        if self.presence_penalty is not None:
            request_kwargs["presence_penalty"] = self.presence_penalty
        
        # Call sglang (only use OpenAI-compatible parameters)
        response = self.client.chat.completions.create(**request_kwargs)
        
        return SamplerResponse(
            response_text=response.choices[0].message.content,
            response_metadata={"usage": response.usage},
            actual_queried_message_list=messages,
        )


# Preset configurations
PRESETS = {
    "original": {
        "model_name": "Qwen3-4B-Instruct-2507",
        "description": "Original BF16 model"
    },
    # ==================== W8A16 Methods ====================
    "w8a16_ptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-PTQ",
        "description": "Simple PTQ W8A16 (baseline)"
    },
    "w8a16_gptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-GPTQ",
        "description": "GPTQ W8A16"
    },
    "w8a16_awq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-AWQ",
        "description": "AWQ W8A16"
    },
    "w8a16_sparse_gptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-SPARSE-GPTQ",
        "description": "SparseGPT â†’ GPTQ W8A16"
    },
    "w8a16_sparse_awq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-SPARSE-AWQ",
        "description": "SparseGPT â†’ AWQ W8A16"
    },
    "w8a16_smooth_gptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-SMOOTH-GPTQ",
        "description": "SmoothQuant + GPTQ W8A16 (comparison)"
    },
    "w8a16_smooth_ptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-SMOOTH-PTQ",
        "description": "SmoothQuant + PTQ W8A16 (complete experiment matrix)"
    },
    "w8a16_smooth_awq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A16-SMOOTH-AWQ",
        "description": "SmoothQuant + AWQ W8A16 (comparison)"
    },
    # ==================== W8A8 Methods (Priority) ====================
    "w8a8_smooth_gptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A8-SMOOTH-GPTQ",
        "description": "â­ SmoothQuant + GPTQ W8A8 (priority)"
    },
    "w8a8_smooth_awq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A8-SMOOTH-AWQ",
        "description": "â­ SmoothQuant + AWQ W8A8 (priority)"
    },
    "w8a8_sparse_smooth_gptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A8-SPARSE-SMOOTH-GPTQ",
        "description": "SparseGPT â†’ SmoothQuant + GPTQ W8A8 (memory efficient)"
    },
    "w8a8_smooth_ptq": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A8-SMOOTH-PTQ",
        "description": "SmoothQuant + PTQ W8A8 (fast baseline)"
    },
    "w8a8_awq_smooth": {
        "model_name": "Qwen3-4B-Instruct-2507-INT8-W8A8-AWQ-LIGHTSMOOTH",
        "description": "AWQ + Light SmoothQuant W8A8"
    }
}


def main():
    # æ„å»ºé¢„è®¾æ¨¡å‹çš„å¸®åŠ©ä¿¡æ¯
    preset_help = "ä½¿ç”¨é¢„è®¾æ¨¡å‹: " + ", ".join([
        f"{k} ({v['description']})" for k, v in PRESETS.items()
    ])
    
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ sglang åç«¯è¿è¡Œ GPQA è¯„ä¼°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é¢„è®¾é…ç½®
  python run_gpqa_sglang.py --model original
  python run_gpqa_sglang.py --model w8a16
  
  # è‡ªå®šä¹‰é…ç½®
  python run_gpqa_sglang.py \\
      --model-name MyModel \\
      --variant extended \\
      --max-tokens 32768
        """
    )
    
    # é¢„è®¾æˆ–è‡ªå®šä¹‰æ¨¡å‹
    parser.add_argument(
        "--model",
        type=str,
        choices=list(PRESETS.keys()),
        help=preset_help
    )
    parser.add_argument(
        "--model-name",
        type=str,
        help="è‡ªå®šä¹‰æ¨¡å‹åç§°ï¼ˆè¦†ç›– --modelï¼‰"
    )
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument(
        "--base-url",
        type=str,
        default="http://127.0.0.1:30000/v1",
        help="Sglang æœåŠ¡å™¨åœ°å€ (é»˜è®¤: http://127.0.0.1:30000/v1)"
    )
    
    # è¯„ä¼°é…ç½®
    parser.add_argument(
        "--variant",
        type=str,
        default="diamond",
        choices=["diamond", "extended", "main"],
        help="GPQA å˜ä½“ (é»˜è®¤: diamond)"
    )
    parser.add_argument(
        "--num-examples",
        type=int,
        default=None,
        help="æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNone = å…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--n-repeats",
        type=int,
        default=1,
        help="æ¯ä¸ªæ ·æœ¬é‡å¤æ¬¡æ•°ï¼ˆä»…å½“ num-examples=None æ—¶æ”¯æŒ >1ï¼‰"
    )
    parser.add_argument(
        "--n-shot",
        type=int,
        default=0,
        help="Few-shot ç¤ºä¾‹æ•°é‡ (0=zero-shot, é»˜è®¤: 0)"
    )
    
    # é‡‡æ ·å‚æ•°
    parser.add_argument(
        "--greedy",
        action="store_true",
        help="å¯ç”¨ Greedy è§£ç æ¨¡å¼ (Temperature=0.0, TopP=0.8)ã€‚é»˜è®¤ä½¿ç”¨ Do-sample æ¨¡å¼ (Temperature=0.7, TopP=0.8)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=None,
        help="é‡‡æ ·æ¸©åº¦ (é»˜è®¤: greedy=True æ—¶ä¸º 0.0, greedy=False æ—¶ä¸º 0.7)"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=16384,
        help="æœ€å¤§ç”Ÿæˆ token æ•° (é»˜è®¤: 16384 = 16k)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=1234,
        help="éšæœºç§å­ï¼Œç”¨äºç¡®ä¿ç»“æœå¯é‡å¤ (é»˜è®¤: 1234)"
    )
    
    # è¾“å‡ºé…ç½®
    parser.add_argument(
        "--config-name",
        type=str,
        default=None,
        help="å¯é€‰çš„é…ç½®åç§°ï¼Œç”¨äºè¿›ä¸€æ­¥åŒºåˆ†å®éªŒï¼Œå¦‚ 'qwen_prompt', 'ablation_study' ç­‰ã€‚ä¸æä¾›åˆ™è‡ªåŠ¨ç”ŸæˆåŸºç¡€é…ç½®å"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="results",
        help="ç»“æœä¿å­˜ç›®å½• (é»˜è®¤: results/)"
    )
    
    args = parser.parse_args()
    
    # å¤„ç†é‡‡æ ·å‚æ•°
    if args.greedy:
        # Greedy æ¨¡å¼ï¼šæ˜¾å¼è¦†ç›–æ¨¡å‹é»˜è®¤å‚æ•°
        temperature = args.temperature if args.temperature is not None else 0.0
        top_p = 0.8
        presence_penalty = 0.0
    else:
        # Do-sample æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„é»˜è®¤å‚æ•°ï¼ˆä¸è¦†ç›–ï¼‰
        # æ¨¡å‹é»˜è®¤: temperature=0.7, top_k=20, top_p=0.8
        temperature = None
        top_p = None
        presence_penalty = None
    
    # ç¡®å®šæ¨¡å‹åç§°
    if args.model:
        model_name = PRESETS[args.model]["model_name"]
        preset_desc = PRESETS[args.model]["description"]
    elif args.model_name:
        model_name = args.model_name
        preset_desc = "è‡ªå®šä¹‰æ¨¡å‹"
    else:
        parser.error("å¿…é¡»æŒ‡å®š --model æˆ– --model-name")
    
    # æ‰“å°é…ç½®
    print(f"\n{'='*70}")
    print(f"ğŸš€ GPQA è¯„ä¼°é…ç½®")
    print(f"{'='*70}")
    print(f"æ¨¡å‹: {model_name}")
    print(f"      ({preset_desc})")
    print(f"æœåŠ¡å™¨: {args.base_url}")
    print(f"å˜ä½“: {args.variant}")
    print(f"æ ·æœ¬æ•°: {args.num_examples or 'ALL'} Ã— {args.n_repeats} repeats")
    shot_mode = f"{args.n_shot}-shot" if args.n_shot > 0 else "Zero-shot"
    
    if args.greedy:
        sampling_mode = "Greedy" if temperature == 0.0 else f"Greedy (Temp={temperature})"
        sampling_detail = f"Temp={temperature}, TopP={top_p}"
    else:
        sampling_mode = "DoSample (ä½¿ç”¨æ¨¡å‹é»˜è®¤å‚æ•°)"
        sampling_detail = "Temp=0.7, TopK=20, TopP=0.8 (æ¨¡å‹é»˜è®¤)"
    
    print(f"é‡‡æ ·: {shot_mode}, {sampling_mode}, Max-Tokens={args.max_tokens}, Seed={args.seed}")
    print(f"      å‚æ•°: {sampling_detail}")
    
    # æå‰è®¡ç®—é…ç½®åç§°ç”¨äºæ˜¾ç¤º
    sampling_part_preview = "greedy" if args.greedy else "dosample"
    shot_part_preview = f"{args.n_shot}shot" if args.n_shot > 0 else "zeroshot"
    repeat_part_preview = f"{args.n_repeats}repeat"
    config_preview = f"{sampling_part_preview}_{shot_part_preview}_{repeat_part_preview}"
    if args.num_examples:
        config_preview += f"_{args.num_examples}samples"
    if args.config_name:
        config_preview += f"_{args.config_name}"
        print(f"é…ç½®åç§°: {config_preview}")
        print(f"          (è‡ªåŠ¨: {sampling_part_preview}_{shot_part_preview}_{repeat_part_preview} + è‡ªå®šä¹‰: {args.config_name})")
    else:
        print(f"é…ç½®åç§°: {config_preview} (è‡ªåŠ¨ç”Ÿæˆ)")
    
    print(f"è¾“å‡º: {args.output_dir}/")
    print(f"{'='*70}\n")
    
    # åˆ›å»º Sampler
    sampler = SglangSampler(
        base_url=args.base_url,
        temperature=temperature,
        top_p=top_p,
        presence_penalty=presence_penalty,
        max_tokens=args.max_tokens,
        seed=args.seed
    )
    
    # æµ‹è¯•è¿æ¥
    print("ğŸ”Œ æµ‹è¯•è¿æ¥...")
    try:
        test_response = sampler([{"role": "user", "content": "Hello"}])
        print(f"âœ… è¿æ¥æˆåŠŸï¼ˆå“åº”: {test_response.response_text[:50]}...ï¼‰\n")
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        print(f"   è¯·ç¡®ä¿ sglang æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ:")
        print(f"   python -m sglang.launch_server \\")
        print(f"       --model-path /path/to/model \\")
        print(f"       --host 127.0.0.1 \\")
        print(f"       --port 30000")
        sys.exit(1)
    
    # åŠ è½½ GPQA å¹¶å¼€å§‹è¯„ä¼°
    print(f"ğŸ“š åŠ è½½ GPQA ({args.variant}) å¹¶å¼€å§‹è¯„ä¼°...\n")
    
    gpqa_eval = GPQAEval(
        n_repeats=args.n_repeats,
        variant=args.variant,
        num_examples=args.num_examples,
        n_shot=args.n_shot
    )
    
    # è¿è¡Œè¯„ä¼°
    result = gpqa_eval(sampler)
    
    # è‡ªåŠ¨ç”ŸæˆåŸºç¡€é…ç½®å
    # æ ¼å¼: <é‡‡æ ·æ¨¡å¼>_<few-shot>_<n_repeat>[_è‡ªå®šä¹‰åç§°]
    sampling_part = "greedy" if args.greedy else "dosample"
    shot_part = f"{args.n_shot}shot" if args.n_shot > 0 else "zeroshot"
    repeat_part = f"{args.n_repeats}repeat"  # å§‹ç»ˆæ˜¾ç¤º repeat
    
    # åŸºç¡€é…ç½®å
    auto_config_name = f"{sampling_part}_{shot_part}_{repeat_part}"
    
    # å¦‚æœ num_examples è¢«æŒ‡å®šï¼Œä¹ŸåŠ å…¥åŸºç¡€é…ç½®å
    if args.num_examples:
        auto_config_name += f"_{args.num_examples}samples"
    
    # å¦‚æœæä¾›äº† config_nameï¼Œé™„åŠ åˆ°è‡ªåŠ¨ç”Ÿæˆçš„åç§°åé¢
    if args.config_name:
        final_config_name = f"{auto_config_name}_{args.config_name}"
    else:
        final_config_name = auto_config_name
    
    # æ„å»ºç»“æœæ–‡ä»¶åï¼ˆåŒ…å«è¯¦ç»†é…ç½®ä¿¡æ¯ï¼‰
    # ä¾‹å¦‚: results_5shots_dosample_10repeats_seed1234.json
    filename_parts = []
    
    # n_shot
    if args.n_shot > 0:
        filename_parts.append(f"{args.n_shot}shots")
    else:
        filename_parts.append("0shot")
    
    # greedy / dosample
    filename_parts.append(sampling_part)
    
    # n_repeats (å§‹ç»ˆæ˜¾ç¤º)
    filename_parts.append(f"{args.n_repeats}repeats")
    
    # seed (å¦‚æœä¸æ˜¯é»˜è®¤å€¼1234)
    if args.seed != 1234:
        filename_parts.append(f"seed{args.seed}")
    
    # max_tokens (å¦‚æœä¸æ˜¯é»˜è®¤å€¼16384)
    if args.max_tokens != 16384:
        filename_parts.append(f"{args.max_tokens}tokens")
    
    # num_examples (å¦‚æœæŒ‡å®šäº†)
    if args.num_examples:
        filename_parts.append(f"{args.num_examples}samples")
    
    filename_suffix = "_".join(filename_parts)
    
    # ä¿å­˜ç»“æœ - æŒ‰æ¨¡å‹åç§°ã€å˜ä½“ã€æœ€ç»ˆé…ç½®åç»„ç»‡åˆ°å­æ–‡ä»¶å¤¹
    # ç»“æ„: results/æ¨¡å‹å/gpqa_å˜ä½“/æœ€ç»ˆé…ç½®å/results_*.html
    # æœ€ç»ˆé…ç½®å = è‡ªåŠ¨ç”Ÿæˆ_[å¯é€‰è‡ªå®šä¹‰åç§°]
    variant_dir_name = f"gpqa_{args.variant}"
    result_dir = Path(args.output_dir) / model_name / variant_dir_name / final_config_name
    result_dir.mkdir(parents=True, exist_ok=True)
    
    html_file = result_dir / f"results_{filename_suffix}.html"
    json_file = result_dir / f"results_{filename_suffix}.json"
    
    html_file.write_text(common.make_report(result))
    
    # æ„å»ºé…ç½®å­—å…¸
    config_dict = {
        "variant": args.variant,
        "n_repeats": args.n_repeats,
        "num_examples": args.num_examples,
        "n_shot": args.n_shot,
        "greedy": args.greedy,
        "max_tokens": args.max_tokens,
        "seed": args.seed,
    }
    
    # è®°å½•å®é™…ä½¿ç”¨çš„é‡‡æ ·å‚æ•°
    if args.greedy:
        # Greedy æ¨¡å¼ï¼šæ˜¾å¼è¦†ç›–çš„å‚æ•°
        config_dict.update({
            "temperature": temperature,
            "top_p": top_p,
            "presence_penalty": presence_penalty,
        })
    else:
        # Do-sample æ¨¡å¼ï¼šä½¿ç”¨æ¨¡å‹é»˜è®¤å‚æ•°
        config_dict.update({
            "temperature": "model_default (0.7)",
            "top_k": "model_default (20)",
            "top_p": "model_default (0.8)",
            "note": "ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„é»˜è®¤é‡‡æ ·å‚æ•°"
        })
    
    # æ„å»ºå®Œæ•´çš„ JSON è¾“å‡º
    json_output = {
        "model": model_name,
        "config_name": final_config_name,  # æœ€ç»ˆé…ç½®åï¼ˆåŒ…å«è‡ªå®šä¹‰éƒ¨åˆ†ï¼‰
        "auto_config_name": auto_config_name,  # è‡ªåŠ¨ç”Ÿæˆçš„åŸºç¡€éƒ¨åˆ†
        "score": result.score,
        "metrics": result.metrics,
        "config": config_dict
    }
    
    # å¦‚æœæä¾›äº†è‡ªå®šä¹‰ config_nameï¼Œä¹Ÿå•ç‹¬è®°å½•
    if args.config_name:
        json_output["custom_config_suffix"] = args.config_name
    
    json_file.write_text(json.dumps(json_output, indent=2))
    
    # æ‰“å°ç»“æœ
    print(f"\n{'='*70}")
    print(f"ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"å‡†ç¡®ç‡: {result.score:.4f} ({result.score*100:.2f}%)")
    print(f"ç»Ÿè®¡æŒ‡æ ‡æ•°: {len(result.metrics)} ä¸ª")
    print(f"é…ç½®åç§°: {final_config_name}")
    if args.config_name:
        print(f"  (åŸºç¡€: {auto_config_name} + è‡ªå®šä¹‰: {args.config_name})")
    print(f"è¾“å‡ºç›®å½•: {result_dir}/")
    print(f"  â”œâ”€ {html_file.name}")
    print(f"  â””â”€ {json_file.name}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()


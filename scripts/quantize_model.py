#!/usr/bin/env python3
"""
é‡åŒ– Qwen3-4B-Instruct-2507 æ¨¡å‹åˆ° INT8
"""
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from llmcompressor.transformers import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier
from llmcompressor.modifiers.smoothquant import SmoothQuantModifier

# é…ç½®
MODEL_PATH = "/data/jisenli2/huggingface/models--Qwen--Qwen3-4B-Instruct-2507/snapshots/cdbee75f17c01a7cc42f958dc650907174af0554"
MODEL_ID = "Qwen/Qwen3-4B-Instruct-2507"
OUTPUT_DIR = "/data/jisenli2/huggingface/Qwen3-4B-Instruct-2507-INT8-W8A8"

NUM_CALIBRATION_SAMPLES = 512
MAX_SEQUENCE_LENGTH = 2048

def main():
    print("=" * 60)
    print("æ­¥éª¤ 1/4: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
    print("=" * 60)
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        device_map="auto",
        torch_dtype="auto",
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    
    print(f"âœ… æ¨¡å‹å·²åŠ è½½: {MODEL_ID}")
    print(f"âœ… åˆ†è¯å™¨å·²åŠ è½½")
    
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 2/4: å‡†å¤‡æ ¡å‡†æ•°æ®")
    print("=" * 60)
    
    # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
    ds = load_dataset("HuggingFaceH4/ultrachat_200k", split="train_sft")
    ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))
    
    def preprocess(example):
        return {"text": tokenizer.apply_chat_template(example["messages"], tokenize=False)}
    
    ds = ds.map(preprocess)
    
    def tokenize(sample):
        return tokenizer(
            sample["text"], 
            padding=False, 
            max_length=MAX_SEQUENCE_LENGTH, 
            truncation=True, 
            add_special_tokens=False
        )
    
    ds = ds.map(tokenize, remove_columns=ds.column_names)
    
    print(f"âœ… æ ¡å‡†æ•°æ®å·²å‡†å¤‡: {NUM_CALIBRATION_SAMPLES} æ ·æœ¬")
    
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 3/4: åº”ç”¨ INT8 é‡åŒ– (W8A8)")
    print("=" * 60)
    print("â³ è¿™å¯èƒ½éœ€è¦ 30-60 åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # é…ç½®é‡åŒ–ç®—æ³•
    recipe = [
        SmoothQuantModifier(smoothing_strength=0.8),
        GPTQModifier(targets="Linear", scheme="W8A8", ignore=["lm_head"]),
    ]
    
    # åº”ç”¨é‡åŒ–
    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe,
        max_seq_length=MAX_SEQUENCE_LENGTH,
        num_calibration_samples=NUM_CALIBRATION_SAMPLES,
    )
    
    print("âœ… é‡åŒ–å®Œæˆ")
    
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 4/4: ä¿å­˜é‡åŒ–æ¨¡å‹")
    print("=" * 60)
    
    model.save_pretrained(OUTPUT_DIR, save_compressed=True)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"âœ… é‡åŒ–æ¨¡å‹å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    print("\n" + "=" * 60)
    print("ğŸ‰ é‡åŒ–æµç¨‹å®Œæˆ!")
    print("=" * 60)
    print(f"ğŸ“‚ åŸå§‹æ¨¡å‹: {MODEL_PATH}")
    print(f"ğŸ“‚ é‡åŒ–æ¨¡å‹: {OUTPUT_DIR}")
    print(f"ğŸ“Š é‡åŒ–æ–¹æ¡ˆ: W8A8 (æƒé‡å’Œæ¿€æ´»å‡ä¸º INT8)")

if __name__ == "__main__":
    main()

